import argparse
import random

# Define constants for common SWF status codes (you can adjust these)
SWF_STATUS_CODES = {
    "completed": 1,
    "failed": 5,        # Example: Job failed for general reason
    "killed": 6,        # Example: Job killed by system/user
    "timeout": 7,       # Example: Job timed out
    "cancelled": 8,     # Example: Job was cancelled
    "node_failure": 9   # Example: Job failed due to node issue
}


def generate_swf(num_jobs, job_id_start,
                submit_time_start, submit_time_step,
                wait_time_default,
                run_time_min, run_time_max,
                min_procs, max_procs,
                avg_cpu_frac,
                mem_min_kb, mem_max_kb,
                status_choices, # Original default status choices
                user_id_min, user_id_max,
                group_id_min, group_id_max,
                executable_min, executable_max,
                queue_default, partition_default,
                preceding_job_default,
                think_time_default,
                output_file,
                # New failure parameters
                general_failure_range,
                timeout_failure_range,
                cancel_failure_range):

    submit_time = submit_time_start
    jobs_data = []

    # Generate initial job data
    for i in range(num_jobs):
        job_id = job_id_start + i
        jobs_data.append({
            "job_id": job_id,
            "submit_time": submit_time,
            "wait_time": wait_time_default,
            "run_time": random.randint(run_time_min, run_time_max),
            "num_procs": random.randint(min_procs, max_procs),
            "avg_cpu_time": 0,  # Will be calculated later
            "used_memory": random.randint(mem_min_kb, mem_max_kb),
            "requested_procs": 0, # Will be set to num_procs
            "requested_time": 0,  # Will be set to run_time
            "requested_memory": 0, # Will be set to used_memory
            "status": random.choice(status_choices), # Default status
            "user_id": random.randint(user_id_min, user_id_max),
            "group_id": random.randint(group_id_min, group_id_max),
            "executable": random.randint(executable_min, executable_max),
            "queue": queue_default,
            "partition": partition_default,
            "preceding_job": preceding_job_default,
            "think_time": think_time_default
        })
        submit_time += submit_time_step

    # Inject failures
    num_jobs_to_fail = min(int(num_jobs * random.uniform(*general_failure_range) / 100), num_jobs)
    num_jobs_to_timeout = min(int(num_jobs * random.uniform(*timeout_failure_range) / 100), num_jobs)
    num_jobs_to_cancel = min(int(num_jobs * random.uniform(*cancel_failure_range) / 100), num_jobs)

    # Get indices of jobs that are initially considered 'completed' for failure injection
    # This prevents overriding statuses if status_choices already includes failure states
    eligible_indices_for_failure = [i for i, job in enumerate(jobs_data) if job["status"] in status_choices]

    random.shuffle(eligible_indices_for_failure)

    failure_counts = {
        "general": 0,
        "timeout": 0,
        "cancelled": 0,
        "completed": 0
    }

    # Inject general failures
    for i in range(min(num_jobs_to_fail, len(eligible_indices_for_failure))):
        idx = eligible_indices_for_failure.pop(0) # Remove so we don't re-select
        jobs_data[idx]["status"] = SWF_STATUS_CODES["failed"]
        failure_counts["general"] += 1

    # Inject timeouts (from remaining eligible jobs)
    for i in range(min(num_jobs_to_timeout, len(eligible_indices_for_failure))):
        idx = eligible_indices_for_failure.pop(0)
        jobs_data[idx]["status"] = SWF_STATUS_CODES["timeout"]
        failure_counts["timeout"] += 1

    # Inject cancellations (from remaining eligible jobs)
    for i in range(min(num_jobs_to_cancel, len(eligible_indices_for_failure))):
        idx = eligible_indices_for_failure.pop(0)
        jobs_data[idx]["status"] = SWF_STATUS_CODES["cancelled"]
        failure_counts["cancelled"] += 1

    # Calculate final derived fields and count completed jobs
    for job in jobs_data:
        job["avg_cpu_time"] = job["run_time"] * avg_cpu_frac
        job["requested_procs"] = job["num_procs"]
        job["requested_time"] = job["run_time"]
        job["requested_memory"] = job["used_memory"]
        if job["status"] == SWF_STATUS_CODES["completed"]:
            failure_counts["completed"] += 1


    with open(output_file, 'w') as f:
        f.write("; Synthetic SWF workload generated by script\n")
        f.write("; Fields:\n")
        f.write("; 1.JobID 2.SubmitTime 3.WaitTime 4.RunTime 5.NumAllocatedProcessors\n")
        f.write("; 6.AvgCPUTimeUsed 7.UsedMemory 8.RequestedProcessors 9.RequestedTime\n")
        f.write("; 10.RequestedMemory 11.Status 12.UserID 13.GroupID 14.Executable\n")
        f.write("; 15.Queue 16.Partition 17.PrecedingJob 18.ThinkTime\n")

        for job in jobs_data:
            line = f"{job['job_id']} {job['submit_time']} {job['wait_time']} {job['run_time']} {job['num_procs']} " \
                    f"{job['avg_cpu_time']:.2f} {job['used_memory']} {job['requested_procs']} {job['requested_time']} {job['requested_memory']} " \
                    f"{job['status']} {job['user_id']} {job['group_id']} {job['executable']} {job['queue']} {job['partition']} {job['preceding_job']} {job['think_time']}\n"
            f.write(line)

    print("\n--- SWF Workload Generation Summary ---")
    print(f"Total jobs generated: {num_jobs}")
    print(f"General failures: {failure_counts['general']}")
    print(f"Timeouts: {failure_counts['timeout']}")
    print(f"Cancelled jobs: {failure_counts['cancelled']}")
    print(f"Completed jobs: {failure_counts['completed']}")
    print(f"Output saved to: {output_file}")
    print("---------------------------------------\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate a customizable synthetic SWF workload file with failure injection.")

    parser.add_argument("--num_jobs", type=int, default=100, help="Number of jobs to generate")
    parser.add_argument("--job_id_start", type=int, default=1, help="Starting job ID")
    parser.add_argument("--submit_time_start", type=int, default=0, help="Submit time for first job")
    parser.add_argument("--submit_time_step", type=int, default=10, help="Increment of submit time between jobs")
    parser.add_argument("--wait_time_default", type=int, default=0, help="Default wait time for all jobs")
    parser.add_argument("--run_time_min", type=int, default=10, help="Minimum run time")
    parser.add_argument("--run_time_max", type=int, default=1000, help="Maximum run time")
    parser.add_argument("--min_procs", type=int, default=1, help="Minimum processors per job")
    parser.add_argument("--max_procs", type=int, default=16, help="Maximum processors per job")
    parser.add_argument("--avg_cpu_frac", type=float, default=0.9, help="Fraction of runtime that is CPU time")
    parser.add_argument("--mem_min_kb", type=int, default=100000, help="Minimum memory used (KB)")
    parser.add_argument("--mem_max_kb", type=int, default=1000000, help="Maximum memory used (KB)")
    # Default status_choices should ideally only be 'completed' for failure injection to work as expected
    parser.add_argument("--status_choices", nargs='+', type=int, default=[SWF_STATUS_CODES["completed"]],
                        help=f"Possible job statuses for *successful* jobs. Default: {[SWF_STATUS_CODES['completed']]}")
    parser.add_argument("--user_id_min", type=int, default=1, help="Minimum user ID")
    parser.add_argument("--user_id_max", type=int, default=10, help="Maximum user ID")
    parser.add_argument("--group_id_min", type=int, default=1, help="Minimum group ID")
    parser.add_argument("--group_id_max", type=int, default=10, help="Maximum group ID")
    parser.add_argument("--executable_min", type=int, default=1, help="Minimum executable ID")
    parser.add_argument("--executable_max", type=int, default=5, help="Maximum executable ID")
    parser.add_argument("--queue_default", type=int, default=1, help="Queue number")
    parser.add_argument("--partition_default", type=int, default=1, help="Partition number")
    parser.add_argument("--preceding_job_default", type=int, default=-1, help="Preceding job ID")
    parser.add_argument("--think_time_default", type=int, default=0, help="Think time after preceding job (seconds)")
    parser.add_argument("--output", type=str, default="synthetic_workload.swf", help="Output SWF file")

    # New arguments for failure rates
    parser.add_argument("--general-failure-min", type=float, default=0.0,
                        help="Minimum percentage of jobs to mark as general failures (0-100)")
    parser.add_argument("--general-failure-max", type=float, default=5.0,
                        help="Maximum percentage of jobs to mark as general failures (0-100)")
    parser.add_argument("--timeout-failure-min", type=float, default=0.0,
                        help="Minimum percentage of jobs to mark as timeouts (0-100)")
    parser.add_argument("--timeout-failure-max", type=float, default=2.0,
                        help="Maximum percentage of jobs to mark as timeouts (0-100)")
    parser.add_argument("--cancel-failure-min", type=float, default=0.0,
                        help="Minimum percentage of jobs to mark as cancelled (0-100)")
    parser.add_argument("--cancel-failure-max", type=float, default=1.0,
                        help="Maximum percentage of jobs to mark as cancelled (0-100)")

    args = parser.parse_args()

    # Ensure status_choices always includes the 'completed' status if failures are enabled
    if SWF_STATUS_CODES["completed"] not in args.status_choices:
        args.status_choices.append(SWF_STATUS_CODES["completed"])


    generate_swf(
        num_jobs=args.num_jobs,
        job_id_start=args.job_id_start,
        submit_time_start=args.submit_time_start,
        submit_time_step=args.submit_time_step,
        wait_time_default=args.wait_time_default,
        run_time_min=args.run_time_min,
        run_time_max=args.run_time_max,
        min_procs=args.min_procs,
        max_procs=args.max_procs,
        avg_cpu_frac=args.avg_cpu_frac,
        mem_min_kb=args.mem_min_kb,
        mem_max_kb=args.mem_max_kb,
        status_choices=args.status_choices,
        user_id_min=args.user_id_min,
        user_id_max=args.user_id_max,
        group_id_min=args.group_id_min,
        group_id_max=args.group_id_max,
        executable_min=args.executable_min,
        executable_max=args.executable_max,
        queue_default=args.queue_default,
        partition_default=args.partition_default,
        preceding_job_default=args.preceding_job_default,
        think_time_default=args.think_time_default,
        output_file=args.output,
        # Pass failure arguments
        general_failure_range=(args.general_failure_min, args.general_failure_max),
        timeout_failure_range=(args.timeout_failure_min, args.timeout_failure_max),
        cancel_failure_range=(args.cancel_failure_min, args.cancel_failure_max)
    )